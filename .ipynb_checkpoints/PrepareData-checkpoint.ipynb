{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the test file to the idea prove.\n",
    "\n",
    "Try to do the Json formatted corpus, but it is so hard, then I find the word2vec can avoid this hard work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import codecs\n",
    "import argparse\n",
    "\n",
    "TIME_FORMAT = '%Y-%m-%d %H:%M:%S'\n",
    "BASE_FOLDER = \"C:/Users/sethf/source/repos/chinesepoem/\" # os.path.abspath(os.path.dirname(__file__))\n",
    "DATA_FOLDER = os.path.join(BASE_FOLDER, 'data')\n",
    "DEFAULT_FIN = os.path.join(DATA_FOLDER, '唐诗语料库.txt')\n",
    "DEFAULT_FOUT = os.path.join(DATA_FOLDER, 'poem.txt')\n",
    "reg_noisy = re.compile('[^\\u3000-\\uffee]')\n",
    "reg_note = re.compile('(（.*）)') # Cannot deal with （） in seperate lines\n",
    "# 中文及全角标点符号(字符)是\\u3000-\\u301e\\ufe10-\\ufe19\\ufe30-\\ufe44\\ufe50-\\ufe6b\\uff01-\\uffee\n",
    "\n",
    "def set_arguments():\n",
    "    parser = argparse.ArgumentParser(description='Pre process')\n",
    "    parser.add_argument('--fin', type=str, default=DEFAULT_FIN,\n",
    "                        help='Input file path, default is {}'.format(DEFAULT_FIN))\n",
    "    parser.add_argument('--fout', type=str, default=DEFAULT_FOUT,\n",
    "                        help='Output file path, default is {}'.format(DEFAULT_FOUT))\n",
    "    return parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-10-15 14:34:54 START\n",
      "2017-10-15 14:34:56 STOP\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  #  parser = set_arguments()\n",
    "  #  cmd_args = parser.parse_args()\n",
    "\n",
    "    print('{} START'.format(time.strftime(TIME_FORMAT)))\n",
    "\n",
    "    fd = codecs.open(DEFAULT_FIN, 'r', 'utf-8')\n",
    "    fw = codecs.open( DEFAULT_FOUT, 'w', 'utf-8')\n",
    "    reg = re.compile('〖(.*)〗')\n",
    "    start_flag = False\n",
    "    for line in fd:\n",
    "        line = line.strip()\n",
    "        if not line or '《全唐诗》' in line or '<http'  in line or '□' in line:\n",
    "            continue\n",
    "        elif '〖' in line and '〗' in line:\n",
    "            if start_flag:\n",
    "                fw.write('\\n')\n",
    "            start_flag = True\n",
    "            g = reg.search(line)\n",
    "            if g:\n",
    "                fw.write(g.group(1))\n",
    "                fw.write('\\n')\n",
    "            else:\n",
    "                # noisy data\n",
    "                print(line)\n",
    "        else:\n",
    "            line = reg_noisy.sub('', line)\n",
    "            line = reg_note.sub('', line)\n",
    "            line = line.replace(' .', '')\n",
    "            fw.write(line)\n",
    "\n",
    "    fd.close()\n",
    "    fw.close()\n",
    "\n",
    "    print('{} STOP'.format(time.strftime(TIME_FORMAT)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "\n",
    "#DEFAULT_FOUT = os.path.join(DATA_FOLDER, 'poem.txt')\n",
    "  \n",
    "#thu1 = thulac.thulac(seg_only=True)   #只进行分词，不进行词性标注\n",
    "#text = thu1.cut(\"我爱北京天安门\", text=True)  #进行一句话分词\n",
    "#print(text)\n",
    "\n",
    "thu1 = thulac.thulac(seg_only=True)  #只进行分词，不进行词性标注\n",
    "thu1.cut_f(DEFAULT_FOUT, outp)  #对input.txt文件内容进行分词，输出到output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-10-15 16:26:15 START\n",
      "Model loaded succeed\n",
      "2017-10-15 16:27:58 STOP\n"
     ]
    }
   ],
   "source": [
    "print('{} START'.format(time.strftime(TIME_FORMAT)))\n",
    "\n",
    "import thulac \n",
    "DEFAULT_Segment = os.path.join(DATA_FOLDER, 'wordsegment.txt')\n",
    "\n",
    "fd = codecs.open(DEFAULT_FOUT, 'r', 'utf-8')\n",
    "fw = codecs.open(DEFAULT_Segment, 'w', 'utf-8')\n",
    "\n",
    "thu1 = thulac.thulac(seg_only=True)   #只进行分词，不进行词性标注\n",
    "\n",
    "\n",
    "for line in fd:\n",
    "    #print(line)\n",
    "    fw.write(thu1.cut(line, text=True))\n",
    "    fw.write('\\n')\n",
    "    \n",
    "fd.close()\n",
    "fw.close()\n",
    "\n",
    "print('{} STOP'.format(time.strftime(TIME_FORMAT)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-10-15 16:30:20 START\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-10-15 16:30:31 STOP\n"
     ]
    }
   ],
   "source": [
    "print('{} START'.format(time.strftime(TIME_FORMAT)))\n",
    "from gensim.models import word2vec\n",
    "\n",
    "\n",
    "#DEFAULT_Segment = os.path.join(DATA_FOLDER, 'wordsegment.txt')\n",
    "DEFAULT_Word2Vec = os.path.join(DATA_FOLDER, 'Word2Vec150.bin')\n",
    "\n",
    "sentences = word2vec.Text8Corpus(DEFAULT_Segment)\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, size=150)\n",
    "\n",
    "#DEFAULT_Segment = os.path.join(DATA_FOLDER, 'wordsegment.txt')\n",
    "model.save(DEFAULT_Word2Vec)\n",
    "\n",
    "print('{} STOP'.format(time.strftime(TIME_FORMAT)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.30962595,  0.16889741, -0.01463027, -0.15809815,  0.09206317,\n",
       "       -0.1456935 ,  0.16657346, -0.16048834,  0.03577007, -0.13513733,\n",
       "       -0.09294472, -0.11723404, -0.12365381, -0.02067957,  0.1038581 ,\n",
       "        0.00641506, -0.0062934 ,  0.23415405,  0.37439978, -0.0564473 ,\n",
       "       -0.23397736, -0.19426669,  0.06946895, -0.3208392 ,  0.19368722,\n",
       "        0.02603251, -0.00743247, -0.22094592,  0.01184341, -0.12694272,\n",
       "       -0.32603887, -0.20273098, -0.07396571,  0.01315944, -0.10838111,\n",
       "       -0.0909251 ,  0.00180263, -0.03625318, -0.2046182 , -0.09922028,\n",
       "        0.34920788,  0.08904874, -0.25203493, -0.09772593, -0.03779411,\n",
       "       -0.17694817,  0.07821831,  0.08035509,  0.25622529, -0.08985876,\n",
       "        0.03270766, -0.19293341, -0.30891556,  0.05773695, -0.03148178,\n",
       "        0.33995509, -0.22352351,  0.09742409,  0.14914362, -0.07318434,\n",
       "        0.03735919, -0.08370081, -0.16495866,  0.14458466, -0.04542416,\n",
       "       -0.24301586,  0.08908165,  0.06313832,  0.0586113 , -0.15221816,\n",
       "        0.06224625,  0.08598434, -0.0115755 , -0.09099659,  0.06226088,\n",
       "       -0.07644724,  0.02220215,  0.07566795,  0.04833851,  0.00838657,\n",
       "       -0.05597517, -0.06397859,  0.03784521,  0.02023427, -0.12724152,\n",
       "       -0.01048566,  0.1487288 ,  0.08827937, -0.17855296,  0.31425136,\n",
       "        0.06090816, -0.16096003, -0.07982934,  0.10440107, -0.04465724,\n",
       "        0.06235282, -0.1461063 ,  0.22972585, -0.02483237,  0.1252525 ,\n",
       "       -0.17958631,  0.04755906,  0.26136953,  0.16259584,  0.11282863,\n",
       "        0.10273369, -0.1521662 , -0.11136056,  0.44112033, -0.1723136 ,\n",
       "        0.08373854,  0.16581547, -0.06470159, -0.14097695,  0.07161622,\n",
       "        0.22370109,  0.26647383,  0.24355215, -0.11299301,  0.14951281,\n",
       "       -0.05022607,  0.196927  , -0.06548793,  0.50461113,  0.18641786,\n",
       "       -0.2149298 , -0.05788758,  0.28251058,  0.14605965,  0.4527784 ,\n",
       "        0.00892602,  0.08880702,  0.16401401, -0.03404955, -0.3267473 ,\n",
       "        0.14250852,  0.20599096,  0.13325472, -0.12572202,  0.02558975,\n",
       "       -0.06050026, -0.09717743, -0.20002677,  0.14861256,  0.22908178,\n",
       "       -0.05484885,  0.08654279,  0.07304503,  0.17076297,  0.38086078], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[u'男']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
