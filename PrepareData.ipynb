{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the test file to the idea prove.\n",
    "\n",
    "Try to do the Json formatted corpus, but it is so hard, then I find the word2vec can avoid this hard work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import codecs\n",
    "import argparse\n",
    "\n",
    "TIME_FORMAT = '%Y-%m-%d %H:%M:%S'\n",
    "BASE_FOLDER = \"C:/Users/sethf/source/repos/chinesepoem/\" # os.path.abspath(os.path.dirname(__file__))\n",
    "DATA_FOLDER = os.path.join(BASE_FOLDER, 'data')\n",
    "DEFAULT_FIN = os.path.join(DATA_FOLDER, '唐诗语料库.txt')\n",
    "DEFAULT_FOUT = os.path.join(DATA_FOLDER, 'poem.txt')\n",
    "reg_noisy = re.compile('[^\\u3000-\\uffee]')\n",
    "reg_note = re.compile('(（.*）)') # Cannot deal with （） in seperate lines\n",
    "# 中文及全角标点符号(字符)是\\u3000-\\u301e\\ufe10-\\ufe19\\ufe30-\\ufe44\\ufe50-\\ufe6b\\uff01-\\uffee\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取数据，去掉不用的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-10-15 14:34:54 START\n",
      "2017-10-15 14:34:56 STOP\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  #  parser = set_arguments()\n",
    "  #  cmd_args = parser.parse_args()\n",
    "\n",
    "    print('{} START'.format(time.strftime(TIME_FORMAT)))\n",
    "\n",
    "    fd = codecs.open(DEFAULT_FIN, 'r', 'utf-8')\n",
    "    fw = codecs.open( DEFAULT_FOUT, 'w', 'utf-8')\n",
    "    reg = re.compile('〖(.*)〗')\n",
    "    start_flag = False\n",
    "    for line in fd:\n",
    "        line = line.strip()\n",
    "        if not line or '《全唐诗》' in line or '<http'  in line or '□' in line:\n",
    "            continue\n",
    "        elif '〖' in line and '〗' in line:\n",
    "            if start_flag:\n",
    "                fw.write('\\n')\n",
    "            start_flag = True\n",
    "            g = reg.search(line)\n",
    "            if g:\n",
    "                fw.write(g.group(1))\n",
    "                fw.write('\\n')\n",
    "            else:a\n",
    "                # noisy data\n",
    "                print(line)\n",
    "        else:\n",
    "            line = reg_noisy.sub('', line)\n",
    "            line = reg_note.sub('', line)\n",
    "            line = line.replace(' .', '')\n",
    "            fw.write(line)\n",
    "\n",
    "    fd.close()\n",
    "    fw.close()\n",
    "\n",
    "    print('{} STOP'.format(time.strftime(TIME_FORMAT)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 分词实验\n",
    "#DEFAULT_FOUT = os.path.join(DATA_FOLDER, 'poem.txt')\n",
    "  \n",
    "#thu1 = thulac.thulac(seg_only=True)   #只进行分词，不进行词性标注\n",
    "#text = thu1.cut(\"我爱北京天安门\", text=True)  #进行一句话分词\n",
    "#print(text)\n",
    "\n",
    "thu1 = thulac.thulac(seg_only=True)  #只进行分词，不进行词性标注\n",
    "thu1.cut_f(DEFAULT_FOUT, outp)  #对input.txt文件内容进行分词，输出到output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-10-15 16:26:15 START\n",
      "Model loaded succeed\n",
      "2017-10-15 16:27:58 STOP\n"
     ]
    }
   ],
   "source": [
    "print('{} START'.format(time.strftime(TIME_FORMAT)))\n",
    "\n",
    "import thulac \n",
    "DEFAULT_Segment = os.path.join(DATA_FOLDER, 'wordsegment.txt')\n",
    "\n",
    "fd = codecs.open(DEFAULT_FOUT, 'r', 'utf-8')\n",
    "fw = codecs.open(DEFAULT_Segment, 'w', 'utf-8')\n",
    "\n",
    "thu1 = thulac.thulac(seg_only=True)   #只进行分词，不进行词性标注\n",
    "\n",
    "\n",
    "for line in fd:\n",
    "    #print(line)\n",
    "    fw.write(thu1.cut(line, text=True))\n",
    "    fw.write('\\n')\n",
    "    \n",
    "fd.close()\n",
    "fw.close()\n",
    "\n",
    "print('{} STOP'.format(time.strftime(TIME_FORMAT)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-10-15 16:30:20 START\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-10-15 16:30:31 STOP\n"
     ]
    }
   ],
   "source": [
    "print('{} START'.format(time.strftime(TIME_FORMAT)))\n",
    "from gensim.models import word2vec\n",
    "\n",
    "\n",
    "#DEFAULT_Segment = os.path.join(DATA_FOLDER, 'wordsegment.txt')\n",
    "DEFAULT_Word2Vec = os.path.join(DATA_FOLDER, 'Word2Vec150.bin')\n",
    "\n",
    "sentences = word2vec.Text8Corpus(DEFAULT_Segment)\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, size=150)\n",
    "\n",
    "#DEFAULT_Segment = os.path.join(DATA_FOLDER, 'wordsegment.txt')\n",
    "model.save(DEFAULT_Word2Vec)\n",
    "\n",
    "print('{} STOP'.format(time.strftime(TIME_FORMAT)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.30962595,  0.16889741, -0.01463027, -0.15809815,  0.09206317,\n",
       "       -0.1456935 ,  0.16657346, -0.16048834,  0.03577007, -0.13513733,\n",
       "       -0.09294472, -0.11723404, -0.12365381, -0.02067957,  0.1038581 ,\n",
       "        0.00641506, -0.0062934 ,  0.23415405,  0.37439978, -0.0564473 ,\n",
       "       -0.23397736, -0.19426669,  0.06946895, -0.3208392 ,  0.19368722,\n",
       "        0.02603251, -0.00743247, -0.22094592,  0.01184341, -0.12694272,\n",
       "       -0.32603887, -0.20273098, -0.07396571,  0.01315944, -0.10838111,\n",
       "       -0.0909251 ,  0.00180263, -0.03625318, -0.2046182 , -0.09922028,\n",
       "        0.34920788,  0.08904874, -0.25203493, -0.09772593, -0.03779411,\n",
       "       -0.17694817,  0.07821831,  0.08035509,  0.25622529, -0.08985876,\n",
       "        0.03270766, -0.19293341, -0.30891556,  0.05773695, -0.03148178,\n",
       "        0.33995509, -0.22352351,  0.09742409,  0.14914362, -0.07318434,\n",
       "        0.03735919, -0.08370081, -0.16495866,  0.14458466, -0.04542416,\n",
       "       -0.24301586,  0.08908165,  0.06313832,  0.0586113 , -0.15221816,\n",
       "        0.06224625,  0.08598434, -0.0115755 , -0.09099659,  0.06226088,\n",
       "       -0.07644724,  0.02220215,  0.07566795,  0.04833851,  0.00838657,\n",
       "       -0.05597517, -0.06397859,  0.03784521,  0.02023427, -0.12724152,\n",
       "       -0.01048566,  0.1487288 ,  0.08827937, -0.17855296,  0.31425136,\n",
       "        0.06090816, -0.16096003, -0.07982934,  0.10440107, -0.04465724,\n",
       "        0.06235282, -0.1461063 ,  0.22972585, -0.02483237,  0.1252525 ,\n",
       "       -0.17958631,  0.04755906,  0.26136953,  0.16259584,  0.11282863,\n",
       "        0.10273369, -0.1521662 , -0.11136056,  0.44112033, -0.1723136 ,\n",
       "        0.08373854,  0.16581547, -0.06470159, -0.14097695,  0.07161622,\n",
       "        0.22370109,  0.26647383,  0.24355215, -0.11299301,  0.14951281,\n",
       "       -0.05022607,  0.196927  , -0.06548793,  0.50461113,  0.18641786,\n",
       "       -0.2149298 , -0.05788758,  0.28251058,  0.14605965,  0.4527784 ,\n",
       "        0.00892602,  0.08880702,  0.16401401, -0.03404955, -0.3267473 ,\n",
       "        0.14250852,  0.20599096,  0.13325472, -0.12572202,  0.02558975,\n",
       "       -0.06050026, -0.09717743, -0.20002677,  0.14861256,  0.22908178,\n",
       "       -0.05484885,  0.08654279,  0.07304503,  0.17076297,  0.38086078], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[u'男']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "饮马长城 窟行\n",
      "\n",
      "塞外 悲风切 ， 交河 冰 已 结 。 瀚海 百重波 ， 阴山 千 里 雪 。 迥戍危 烽火 ， 层峦 引高节 。 悠悠 卷 旆旌 ， 饮马 出 长城 。 寒沙 连 骑迹 ， 朔吹断 边声 。 胡尘清玉塞 ， 羌 笛韵 金钲 。 绝漠 干戈戢 ， 车徒 振 原隰 。 都 尉反龙堆 ， 将 军旋 马邑 。 扬 麾氛 雾静 ， 纪石 功名 立 。 荒裔 一戎衣 ， 灵台 凯歌 入 。\n",
      "\n",
      "饮马长城窟行\n",
      "\n",
      "塞外悲风切，交河冰已结。瀚海百重波，阴山千里雪。迥戍危烽火，层峦引高节。悠悠卷旆旌，饮马出长城。寒沙连骑迹，朔吹断边声。胡尘清玉塞，羌笛韵金钲。绝漠干戈戢，车徒振原隰。都尉反龙堆，将军旋马邑。扬麾氛雾静，纪石功名立。荒裔一戎衣，灵台凯歌入。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_FIN = os.path.join(DATA_FOLDER, '唐诗语料库.txt')\n",
    "DEFAULT_FOUT = os.path.join(DATA_FOLDER, 'poem.txt')\n",
    "DEFAULT_Segment = os.path.join(DATA_FOLDER, 'wordsegment.txt')\n",
    "def GetFirstNline(filePath, linesNumber):\n",
    "    fd = codecs.open(filePath, 'r', 'utf-8')\n",
    "    for i in range(1,linesNumber):\n",
    "        print(fd.readline())\n",
    "    fd.close()\n",
    "\n",
    "GetFirstNline(DEFAULT_Segment, 3)\n",
    "GetFirstNline(DEFAULT_FOUT, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分词不是很成功，我们转向直接用汉字字符来代替分段，我们保留标点符号\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-10-15 17:22:55 START\n",
      "2017-10-15 17:23:02 STOP\n"
     ]
    }
   ],
   "source": [
    "print('{} START'.format(time.strftime(TIME_FORMAT)))\n",
    "\n",
    "DEFAULT_FOUT = os.path.join(DATA_FOLDER, 'poem.txt')\n",
    "DEFAULT_charSegment = os.path.join(DATA_FOLDER, 'Charactersegment.txt')\n",
    "\n",
    "fd = codecs.open(DEFAULT_FOUT, 'r', 'utf-8')\n",
    "fw = codecs.open(DEFAULT_charSegment, 'w', 'utf-8')\n",
    "\n",
    "start_flag = False\n",
    "for line in fd:\n",
    "    if len(line) > 0:\n",
    "        for c in line:\n",
    "            if c != '\\n':\n",
    "                fw.write(c)\n",
    "                fw.write(' ')\n",
    "    fw.write('\\n')\n",
    "\n",
    "fd.close()\n",
    "fw.close()\n",
    "\n",
    "print('{} STOP'.format(time.strftime(TIME_FORMAT)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "饮 马 长 城 窟 行 \n",
      "\n",
      "塞 外 悲 风 切 ， 交 河 冰 已 结 。 瀚 海 百 重 波 ， 阴 山 千 里 雪 。 迥 戍 危 烽 火 ， 层 峦 引 高 节 。 悠 悠 卷 旆 旌 ， 饮 马 出 长 城 。 寒 沙 连 骑 迹 ， 朔 吹 断 边 声 。 胡 尘 清 玉 塞 ， 羌 笛 韵 金 钲 。 绝 漠 干 戈 戢 ， 车 徒 振 原 隰 。 都 尉 反 龙 堆 ， 将 军 旋 马 邑 。 扬 麾 氛 雾 静 ， 纪 石 功 名 立 。 荒 裔 一 戎 衣 ， 灵 台 凯 歌 入 。 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "GetFirstNline(DEFAULT_charSegment, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-10-16 22:17:17 START\n",
      "2017-10-16 22:17:32 STOP\n"
     ]
    }
   ],
   "source": [
    "print('{} START'.format(time.strftime(TIME_FORMAT)))\n",
    "from gensim.models import word2vec\n",
    "\n",
    "\n",
    "#DEFAULT_Segment = os.path.join(DATA_FOLDER, 'wordsegment.txt')\n",
    "DEFAULT_Char2Vec = os.path.join(DATA_FOLDER, 'Char2Vec100.bin')\n",
    "\n",
    "fd = codecs.open(DEFAULT_charSegment, 'r', 'utf-8')\n",
    "\n",
    "sentences = fd.readlines()\n",
    "\n",
    "fd.close\n",
    "\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, size=100)\n",
    "\n",
    "#DEFAULT_Segment = os.path.join(DATA_FOLDER, 'wordsegment.txt')\n",
    "model.save(DEFAULT_Char2Vec)\n",
    "\n",
    "print('{} STOP'.format(time.strftime(TIME_FORMAT)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.2900829 , -0.04809159, -0.46607766, -0.60195959, -0.79692709,\n",
       "        1.45317233, -0.73875636, -0.23516993,  0.52468306, -0.4141095 ,\n",
       "        0.31254441,  0.06157973,  0.52587473,  0.98117661,  0.76936024,\n",
       "        0.17090531,  0.54503411,  0.89224559,  0.63628626, -0.65704244,\n",
       "        0.19324228, -2.19337821, -0.0736718 , -1.12545574,  0.36714867,\n",
       "       -0.23592179,  0.65851527,  1.97759676,  0.0664974 ,  0.34336987,\n",
       "        0.16321452, -0.45230347, -1.16129088, -1.37885571, -0.70058161,\n",
       "       -2.71629333, -0.47714323, -1.35716736, -0.5040586 ,  0.84255946,\n",
       "        0.29387042,  0.96084136,  0.5980038 ,  1.53590572,  0.78642726,\n",
       "       -0.70572197,  2.15199852, -0.09091973,  0.70999056, -1.26367903,\n",
       "       -0.23834354,  0.40385616,  0.76464611, -0.65731245,  0.3340157 ,\n",
       "        0.97213268,  1.46448743,  1.32762229,  0.21536438, -0.69748122,\n",
       "       -1.24047554,  0.52763128,  0.48480916, -0.98241204, -0.71260804,\n",
       "       -0.54136884, -1.04192448,  1.04139686,  0.46493888,  0.94138777,\n",
       "        0.21847701, -0.44784865, -1.06913686, -1.06480539, -0.28641865,\n",
       "       -0.57710785, -0.42219958,  0.06467494,  0.29220659,  0.56308562,\n",
       "       -0.69409251, -1.28817475,  0.24338399, -0.0228632 ,  0.33695638,\n",
       "        0.73314172,  0.78557426,  0.78446829,  0.42267925, -0.7360608 ,\n",
       "       -0.18527743,  0.4405438 ,  1.22639728,  1.25485229,  1.98212445,\n",
       "        0.5071575 , -0.30095363, -0.10453363, -0.94564468,  0.3795009 ], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[u'男']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-10-16 20:25:31 START\n",
      "2017-10-16 20:25:41 STOP\n"
     ]
    }
   ],
   "source": [
    "print('{} START'.format(time.strftime(TIME_FORMAT)))\n",
    "from gensim.models import word2vec\n",
    "\n",
    "DEFAULT_charSegment = os.path.join(DATA_FOLDER, 'Charactersegment.txt')\n",
    "DEFAULT_Char2Vec50 = os.path.join(DATA_FOLDER, 'Char2Vec50.bin')\n",
    "\n",
    "fd = codecs.open(DEFAULT_charSegment, 'r', 'utf-8')\n",
    "\n",
    "sentences = fd.readlines()\n",
    "\n",
    "fd.close\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, size=50)\n",
    "\n",
    "#DEFAULT_Segment = os.path.join(DATA_FOLDER, 'wordsegment.txt')\n",
    "model.save(DEFAULT_Char2Vec50)\n",
    "\n",
    "print('{} STOP'.format(time.strftime(TIME_FORMAT)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('最', 0.7617394924163818),\n",
       " ('爱', 0.7001036405563354),\n",
       " ('共', 0.6234053373336792),\n",
       " ('赏', 0.5743197202682495),\n",
       " (' ', 0.5637354850769043),\n",
       " ('似', 0.560402512550354),\n",
       " ('近', 0.5548217296600342),\n",
       " ('谢', 0.5457607507705688),\n",
       " ('伴', 0.5440549850463867),\n",
       " ('待', 0.5435962677001953)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar([u'好'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
